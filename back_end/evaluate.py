# evaluate.py
import json
import re
import csv
from collections import defaultdict
from handler import query_handler
from typing import List

with open("test_cases.json", "r", encoding="utf-8") as f:
    TEST_CASES = json.load(f)

def normalize_answer(text: str) -> set:
    if not text:
        return set()
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    return set(text.split())

def answer_f1(pred: str, gold: List[str]) -> float:
    pred_tokens = normalize_answer(pred)
    gold_tokens = normalize_answer(" ".join(gold))
    if not gold_tokens:
        return 1.0 if not pred_tokens else 0.0
    if not pred_tokens:
        return 0.0
    common = pred_tokens & gold_tokens
    precision = len(common) / len(pred_tokens)
    recall = len(common) / len(gold_tokens)
    return 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0

def classify_error(question: str, golden: List[str], system_pred: List[str], matched: bool) -> str:
    if not matched:
        return "pattern_mismatch"
    if not system_pred:
        return "kg_missing"  # KG ä¸­æ— æ­¤ä¸‰å…ƒç»„
    if set(system_pred) == set(golden):
        return "correct"
    return "wrong_retrieval"

def get_relation_type(question: str) -> str:
    if "ä½œè¯" in question:
        return "ä½œè¯"
    elif "å”±" in question or "æ­Œæ‰‹" in question:
        return "æ­Œæ‰‹"
    else:
        return "å…¶ä»–"

def evaluate():
    total = len(TEST_CASES)
    f1_total = 0.0
    hits_at_1 = 0
    hdr_numerator = 0
    hdr_denominator = 0

    error_stats = defaultdict(int)
    relation_stats = {"æ­Œæ‰‹": {"p":0, "r":0, "f1":0, "count":0}, "ä½œè¯": {"p":0, "r":0, "f1":0, "count":0}}

    # å­˜å‚¨æ¯æ¡ç»“æœç”¨äºå†™å…¥ CSV
    results_rows = []

    for test_case in TEST_CASES:
        question = test_case["question"]
        golden = test_case["golden_answer"]
        llm_ans = test_case["llm_answer"]

        # è°ƒç”¨ä½ çš„ç³»ç»Ÿ
        print(f"\n[è¯„ä¼°ä¸­] è°ƒç”¨ query_handler å¤„ç†é—®é¢˜: {question}")
        res = query_handler(question)
        print(f"[è¯„ä¼°ä¸­] è¿”å›ç»“æœ: {res}")
        system_ans = res["data"] if res["state"] == 0 else []
        final_str = ", ".join(system_ans)

        # åˆ¤æ–­æ˜¯å¦åŒ¹é…æˆåŠŸï¼ˆæ¨¡æ‹Ÿ handler å†…éƒ¨é€»è¾‘ï¼‰
        matched = any([
            re.search(r"æ­Œæ›²(.+)çš„ä½œè¯äººæ˜¯", question),
            re.search(r"(.+)æ˜¯è°å”±çš„", question),
            re.search(r"è°å”±çš„(.+)", question),
            re.search(r"è°ä½œè¯çš„(.+)", question),
            re.search(r"(.+)æ˜¯å“ªä¸ªä¸“è¾‘çš„", question),  # æ–°å¢ä¸“è¾‘ pattern
        ])

        # Answer F1
        f1 = answer_f1(final_str, golden)
        f1_total += f1

        # Hits@1
        if system_ans and set(system_ans) & set(golden):
            hits_at_1 += 1

        # HDR
        llm_correct = set(normalize_answer(llm_ans)) >= set([g.lower() for g in golden])
        if not llm_correct:
            hdr_denominator += 1
            if set(system_ans) >= set(golden):
                hdr_numerator += 1

        # é”™è¯¯åˆ†ç±»
        err_type = classify_error(question, golden, system_ans, matched)
        error_stats[err_type] += 1

        # å…³ç³»ç±»å‹
        rel = get_relation_type(question)
        if rel in relation_stats:
            relation_stats[rel]["count"] += 1
            relation_stats[rel]["f1"] += f1

        # è®°å½•æœ¬æ¡ç»“æœ
        results_rows.append({
            "question": question,
            "golden_answer": "; ".join(golden) if golden else "",
            "llm_answer": llm_ans,
            "system_answer": "; ".join(system_ans),
            "f1_score": round(f1, 4),
            "error_type": err_type,
            "relation_type": rel
        })

    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    avg_f1 = f1_total / total * 100
    hits_at_1_rate = hits_at_1 / total * 100
    hdr = hdr_numerator / hdr_denominator * 100 if hdr_denominator > 0 else 0.0

    # è¾“å‡ºåˆ°æ§åˆ¶å°
    print("\nğŸ“Š å®˜æ–¹è¯„ä¼°æŒ‡æ ‡ (Academic Standard):")
    print(f"   â€¢ Answer F1 Score : {avg_f1:6.2f}%")
    print(f"   â€¢ KG Hits@1       : {hits_at_1_rate:6.2f}%")
    print(f"   â€¢ Hallucination Correction Rate (HDR): {hdr:6.2f}%\n")

    print("ğŸ” é”™è¯¯åˆ†æ:")
    for err, count in error_stats.items():
        print(f"   â€¢ {err:20s}: {count} ({count/total*100:5.1f}%)")

    print("\nğŸ“ˆ æŒ‰å…³ç³»ç±»å‹è¡¨ç°:")
    for rel, stat in relation_stats.items():
        if stat["count"] > 0:
            avg_rel_f1 = stat["f1"] / stat["count"] * 100
            print(f"   â€¢ {rel:4s} F1: {avg_rel_f1:6.2f}% ({stat['count']} samples)")

    # === å†™å…¥ CSV æ–‡ä»¶ ===
    output_file = "evaluation_results.csv"
    with open(output_file, "w", encoding="utf-8-sig", newline="") as csvfile:
        fieldnames = [
            "question",
            "golden_answer",
            "llm_answer",
            "system_answer",
            "f1_score",
            "error_type",
            "relation_type"
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results_rows)

        # å†™å…¥æ±‡æ€»è¡Œï¼ˆå¯é€‰ï¼‰
        writer.writerow({
            "question": "=== SUMMARY ===",
            "golden_answer": "",
            "llm_answer": "",
            "system_answer": "",
            "f1_score": round(avg_f1 / 100, 4),
            "error_type": f"F1={avg_f1:.2f}%, Hits@1={hits_at_1_rate:.2f}%, HDR={hdr:.2f}%",
            "relation_type": ""
        })

    print(f"\nâœ… è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {output_file}")

if __name__ == "__main__":
    evaluate()